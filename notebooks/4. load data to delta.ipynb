{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a6e476-752b-40a6-926f-11ad6e1ef0e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from typing import Any, Mapping\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from dbx_utils.logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Widgets / Inputs\n",
    "# --------------------------------------------------------------------\n",
    "# Same contract as \"3. download api\":\n",
    "# - endpoint_payload: one element from the For Each array (id, endpoint, params, job_settings)\n",
    "# - run_folder: base folder created in step 1, e.g. \"/Volumes/cat/schema/vol/20251210123456\"\n",
    "dbutils.widgets.text(\"endpoint_payload\", \"\")\n",
    "dbutils.widgets.text(\"run_folder\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45dee50-d337-48c2-9efa-075749eb55b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint_payload_raw = dbutils.widgets.get(\"endpoint_payload\")\n",
    "run_folder = dbutils.widgets.get(\"run_folder\")\n",
    "\n",
    "if not endpoint_payload_raw:\n",
    "    raise ValueError(\"Widget 'endpoint_payload' is required (JSON string).\")\n",
    "\n",
    "if not run_folder:\n",
    "    raise ValueError(\"Widget 'run_folder' is required (base volume path for this run).\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Helper: resolve target table from job_settings\n",
    "# --------------------------------------------------------------------\n",
    "def _resolve_target_table(job_settings: Mapping[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Resolve the fully-qualified target table name from job_settings.\n",
    "\n",
    "    Supported patterns (all optional, but at least one must work):\n",
    "\n",
    "    1) Fully-qualified:\n",
    "        job_settings[\"target_table\"] = \"catalog.schema.table\"\n",
    "\n",
    "    2) Split fields:\n",
    "        job_settings[\"target_catalog\"] / job_settings[\"catalog\"]\n",
    "        job_settings[\"target_schema\"]  / job_settings[\"schema\"]\n",
    "        job_settings[\"target_table\"]   / job_settings[\"table\"]\n",
    "\n",
    "    Raises ValueError if a valid full name cannot be constructed.\n",
    "    \"\"\"\n",
    "    # 1) Direct fully-qualified target_table\n",
    "    tt = job_settings.get(\"target_table\")\n",
    "    if isinstance(tt, str) and \".\" in tt:\n",
    "        # Assume user provided \"catalog.schema.table\" or \"schema.table\"\n",
    "        return tt\n",
    "\n",
    "    # 2) Split fields with fallbacks\n",
    "    catalog = job_settings.get(\"target_catalog\") or job_settings.get(\"catalog\")\n",
    "    schema = job_settings.get(\"target_schema\") or job_settings.get(\"schema\")\n",
    "    table = (\n",
    "        job_settings.get(\"target_table\")\n",
    "        or job_settings.get(\"table\")\n",
    "    )\n",
    "\n",
    "    if catalog and schema and table:\n",
    "        return f\"{catalog}.{schema}.{table}\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Could not resolve target Delta table from job_settings. \"\n",
    "        \"Expected either 'target_table' with a qualified name, or \"\n",
    "        \"a combination of target_catalog/catalog, target_schema/schema, \"\n",
    "        \"and target_table/table.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Parse payload\n",
    "# --------------------------------------------------------------------\n",
    "try:\n",
    "    payload: Mapping[str, Any] = json.loads(endpoint_payload_raw)\n",
    "except json.JSONDecodeError as exc:\n",
    "    raise ValueError(\n",
    "        f\"Failed to parse 'endpoint_payload' as JSON: {endpoint_payload_raw!r}\"\n",
    "    ) from exc\n",
    "\n",
    "endpoint_id = payload.get(\"id\")\n",
    "endpoint = payload.get(\"endpoint\")\n",
    "params = payload.get(\"params\") or {}\n",
    "job_settings = payload.get(\"job_settings\") or {}\n",
    "\n",
    "if endpoint_id is None:\n",
    "    raise ValueError(\"Endpoint payload is missing required field 'id'.\")\n",
    "if not endpoint:\n",
    "    raise ValueError(\"Endpoint payload is missing required field 'endpoint'.\")\n",
    "\n",
    "logger.info(\"Starting load-to-Delta for endpoint id=%s\", endpoint_id)\n",
    "logger.info(\"Endpoint URL: %s\", endpoint)\n",
    "logger.info(\"Run folder: %s\", run_folder)\n",
    "\n",
    "# Target Delta table from job_settings\n",
    "target_table = _resolve_target_table(job_settings)\n",
    "logger.info(\"Resolved target Delta table: %s\", target_table)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Determine folder that contains the downloaded JSON files\n",
    "# --------------------------------------------------------------------\n",
    "# Must match the logic used in \"3. download api\"\n",
    "output_subfolder = job_settings.get(\"output_subfolder\") or str(endpoint_id)\n",
    "download_folder = f\"{run_folder}/{output_subfolder}\"\n",
    "\n",
    "logger.info(\n",
    "    \"Using download folder for endpoint id=%s: %s\",\n",
    "    endpoint_id,\n",
    "    download_folder,\n",
    ")\n",
    "\n",
    "# Quick check: does the folder exist and contain anything?\n",
    "try:\n",
    "    files = dbutils.fs.ls(download_folder)\n",
    "except Exception:\n",
    "    logger.warning(\n",
    "        \"Download folder %s does not exist or is not accessible. Skipping.\",\n",
    "        download_folder,\n",
    "    )\n",
    "    dbutils.notebook.exit(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"endpoint_id\": endpoint_id,\n",
    "                \"endpoint\": endpoint,\n",
    "                \"download_folder\": download_folder,\n",
    "                \"target_table\": target_table,\n",
    "                \"status\": \"skipped_missing_folder\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "if not files:\n",
    "    logger.warning(\n",
    "        \"Download folder %s is empty. Nothing to load into %s.\",\n",
    "        download_folder,\n",
    "        target_table,\n",
    "    )\n",
    "    dbutils.notebook.exit(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"endpoint_id\": endpoint_id,\n",
    "                \"endpoint\": endpoint,\n",
    "                \"download_folder\": download_folder,\n",
    "                \"target_table\": target_table,\n",
    "                \"status\": \"skipped_empty_folder\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Read all JSON files as RAW TEXT (one row per file)\n",
    "# --------------------------------------------------------------------\n",
    "# We *intentionally* do NOT try to infer JSON schema here.\n",
    "# We read each file as a single string so the table schema is stable:\n",
    "#   - raw_json: string\n",
    "#   - source_file: string\n",
    "#   - endpoint_id: long\n",
    "#   - endpoint: string\n",
    "#   - params_json: string\n",
    "#   - job_settings_json: string\n",
    "#   - ingested_at: timestamp\n",
    "logger.info(\n",
    "    \"Reading downloaded JSON files as raw text from %s\",\n",
    "    download_folder,\n",
    ")\n",
    "\n",
    "df_raw = (\n",
    "    spark.read.format(\"text\")\n",
    "    .option(\"wholetext\", \"true\")  # one row per file\n",
    "    .load(download_folder)\n",
    "    .withColumnRenamed(\"value\", \"raw_json\")\n",
    ")\n",
    "\n",
    "# Add metadata columns\n",
    "df_raw = (\n",
    "    df_raw\n",
    "    .withColumn(\"source_file\", F.input_file_name())\n",
    "    .withColumn(\"endpoint_id\", F.lit(int(endpoint_id)))\n",
    "    .withColumn(\"endpoint\", F.lit(str(endpoint)))\n",
    "    .withColumn(\"params_json\", F.lit(json.dumps(params)))\n",
    "    .withColumn(\"job_settings_json\", F.lit(json.dumps(job_settings)))\n",
    "    .withColumn(\"ingested_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "row_count = df_raw.count()\n",
    "logger.info(\n",
    "    \"Prepared %d rows for loading into Delta table %s from folder %s\",\n",
    "    row_count,\n",
    "    target_table,\n",
    "    download_folder,\n",
    ")\n",
    "\n",
    "if row_count == 0:\n",
    "    logger.warning(\n",
    "        \"No rows produced from %s after reading as text. Skipping write.\",\n",
    "        download_folder,\n",
    "    )\n",
    "    dbutils.notebook.exit(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"endpoint_id\": endpoint_id,\n",
    "                \"endpoint\": endpoint,\n",
    "                \"download_folder\": download_folder,\n",
    "                \"target_table\": target_table,\n",
    "                \"status\": \"skipped_no_rows\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Write into Delta table (APPEND, stable string-only schema)\n",
    "# --------------------------------------------------------------------\n",
    "logger.info(\n",
    "    \"Writing %d rows into Delta table %s (mode=append)\",\n",
    "    row_count,\n",
    "    target_table,\n",
    ")\n",
    "\n",
    "(\n",
    "    df_raw.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")  # create table if it doesn't exist, append otherwise\n",
    "    .saveAsTable(target_table)\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Completed load-to-Delta for endpoint id=%s into table %s.\",\n",
    "    endpoint_id,\n",
    "    target_table,\n",
    ")\n",
    "\n",
    "result_payload = {\n",
    "    \"endpoint_id\": endpoint_id,\n",
    "    \"endpoint\": endpoint,\n",
    "    \"download_folder\": download_folder,\n",
    "    \"target_table\": target_table,\n",
    "    \"rows_written\": row_count,\n",
    "    \"status\": \"success\",\n",
    "}\n",
    "\n",
    "dbutils.notebook.exit(json.dumps(result_payload))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. load data to delta",
   "widgets": {
    "endpoint_payload": {
     "currentValue": "",
     "nuid": "6f7e4b8b-351b-4d24-9ea6-420ea4c1a788",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "endpoint_payload",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "endpoint_payload",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "run_folder": {
     "currentValue": "",
     "nuid": "963f194f-579a-4f25-bba9-c15695112364",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "run_folder",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "run_folder",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
