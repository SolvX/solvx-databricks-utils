from pyspark.sql import SparkSession
from pyspark.errors import AnalysisException
from dbx_utils.logging import getLogger  # your custom logger


def ingest_setup(
    catalog: str,
    schema: str,
    table: str,
    spark: SparkSession | None = None,
    managed_location: str | None = None,
) -> str:
    """
    Create (if not exists) the API configuration table that stores:
      - endpoint: string
      - params: map<string,string>
      - job_settings: map<string,string>

    This table will be used to define which API endpoints should be ingested.

    The setup only has to run once in total and doesn't need to run every job.

    Parameters
    ----------
    catalog : str
        Unity Catalog name, e.g. "dev".
    schema : str
        Schema name, e.g. "tools".
    table : str
        Table name, e.g. "endpoints".
    spark : SparkSession | None
        Optional SparkSession. If None, the active SparkSession is used
        (as in Databricks notebooks).
    managed_location : str | None
        Optional storage path to use when creating the catalog,
        e.g. 'abfss://container@storage.dfs.core.windows.net/'.
        If your metastore does not have a default storage root, this
        MUST be provided the first time the catalog is created.

    Returns
    -------
    str
        The full table name (catalog.schema.table).
    """
    logger = getLogger(__name__)
    full_table_name = f"{catalog}.{schema}.{table}"

    # Resolve SparkSession
    if spark is None:
        spark = SparkSession.getActiveSession()
        if spark is None:
            raise RuntimeError(
                "No active SparkSession found. "
                "On Databricks this should not happen; "
                "outside Databricks, pass `spark` explicitly to ingest_setup()."
            )

    try:
        logger.info(f"Starting ingest setup for table '{full_table_name}'.")

        # Create catalog (with optional managed location)
        if managed_location:
            logger.info(
                f"Ensuring catalog '{catalog}' exists with managed location "
                f"'{managed_location}'."
            )
            spark.sql(
                f"CREATE CATALOG IF NOT EXISTS {catalog} "
                f"MANAGED LOCATION '{managed_location}'"
            )
        else:
            try:
                logger.info(f"Ensuring catalog '{catalog}' exists.")
                spark.sql(f"CREATE CATALOG IF NOT EXISTS {catalog}")
            except AnalysisException as ae:
                # Give a very explicit hint about the missing storage root
                msg = (
                    f"Failed to create catalog '{catalog}' without a managed location.\n\n"
                    f"This usually means your Unity Catalog metastore does not have a "
                    f"default storage root configured.\n\n"
                    f"Either:\n"
                    f"  • Configure a default storage location for the metastore, OR\n"
                    f"  • Call ingest_setup(...) with catalog_managed_location set, e.g.\n\n"
                    f"    ingest_setup(\n"
                    f"        catalog='{catalog}',\n"
                    f"        schema='{schema}',\n"
                    f"        table='{table}',\n"
                    f"        catalog_managed_location="
                    f"'abfss://container@storage.dfs.core.windows.net/'\n"
                    f"    )\n\n"
                    f"Original error: {ae}"
                )
                logger.error(msg)
                raise RuntimeError(msg) from ae

        # Create schema
        logger.info(f"Ensuring schema '{catalog}.{schema}' exists.")
        spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}")

        # If the table already exists, tell the user explicitly and exit.
        if spark.catalog.tableExists(full_table_name):
            logger.info(
                f"API config table '{full_table_name}' already exists. "
                f"No changes were made. You can safely reuse this table."
            )
            return full_table_name

        # Table does not exist yet: create the flexible config table
        logger.info(f"Creating new API config table '{full_table_name}'.")
        spark.sql(f"""
        CREATE TABLE {full_table_name} (
          id BIGINT GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
          endpoint STRING,
          params MAP<STRING, STRING>,
          job_settings MAP<STRING, STRING>
        )
        USING DELTA
        """)

        logger.info(f"Created new API config table '{full_table_name}'.")
        return full_table_name

    except Exception as e:
        logger.error(
            f"Failed to set up API config table '{full_table_name}': {e}",
            exc_info=True,
        )
        # Re-raise so the job/notebook fails loudly instead of silently
        raise
