from pyspark.sql import SparkSession
from dbx_utils.logging import getLogger  # your custom logger


def ingest_setup(
    catalog: str,
    schema: str,
    table: str,
    spark: SparkSession | None = None,
) -> str:
    """
    Create (if not exists) the API configuration table that stores:
      - endpoint: string
      - params: map<string,string>
      - job_settings: map<string,string>

    This table will be used to define which API endpoints should be ingested.

    The setup only has to run once in total and doesn't need to run every job.

    Parameters
    ----------
    catalog : str
        Unity Catalog catalog name, e.g. "vlad".
    schema : str
        Schema name, e.g. "api".
    table : str
        Table name, e.g. "empire".
    spark : SparkSession | None
        Optional SparkSession. If None, the active SparkSession is used
        (as in Databricks notebooks).

    Returns
    -------
    str
        The full table name (catalog.schema.table).
    """
    logger = getLogger(__name__)
    full_table_name = f"{catalog}.{schema}.{table}"

    # Resolve SparkSession
    if spark is None:
        spark = SparkSession.getActiveSession()
        if spark is None:
            raise RuntimeError(
                "No active SparkSession found. "
                "On Databricks this should not happen; "
                "outside Databricks, pass `spark` explicitly to ingest_setup()."
            )

    try:
        logger.info(f"Starting ingest setup for table '{full_table_name}'.")

        # Create catalog & schema (Unity Catalog safe operations)
        spark.sql(f"CREATE CATALOG IF NOT EXISTS {catalog}")
        spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}")

        # If the table already exists, tell the user explicitly and exit.
        if spark.catalog.tableExists(full_table_name):
            logger.info(
                f"API config table '{full_table_name}' already exists. "
                f"No changes were made. You can safely reuse this table."
            )
            return full_table_name

        # Table does not exist yet: create the flexible config table
        spark.sql(f"""
        CREATE TABLE {full_table_name} (
          id BIGINT GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
          endpoint STRING,
          params MAP<STRING, STRING>,
          job_settings MAP<STRING, STRING>
        )
        USING DELTA
        """)

        logger.info(f"Created new API config table '{full_table_name}'.")
        return full_table_name

    except Exception as e:
        logger.error(
            f"Failed to set up API config table '{full_table_name}': {e}",
            exc_info=True,
        )
        # Re-raise so the job/notebook fails loudly instead of silently
        raise
