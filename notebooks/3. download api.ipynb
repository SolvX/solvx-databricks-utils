{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09beb7a1-9613-42fc-aea0-9bb30a2197f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: 3. download api\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from typing import Any, Mapping\n",
    "\n",
    "import requests\n",
    "\n",
    "from dbx_utils.logging import getLogger\n",
    "from dbx_utils.ingest import download_endpoint_to_volume\n",
    "\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12b3c5b9-4d4f-4c9f-ae87-563b7f534488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Widgets\n",
    "# --------------------------------------------------------------------\n",
    "# One element from the For Each array (JSON dict with id, endpoint, params, job_settings)\n",
    "dbutils.widgets.text(\"endpoint_payload\", \"\")\n",
    "\n",
    "# Base folder created in step 1, e.g. \"/Volumes/cat/schema/vol/20251210123456\"\n",
    "dbutils.widgets.text(\"run_folder\", \"\")\n",
    "\n",
    "endpoint_payload_raw = dbutils.widgets.get(\"endpoint_payload\")\n",
    "run_folder = dbutils.widgets.get(\"run_folder\")\n",
    "\n",
    "if not endpoint_payload_raw:\n",
    "    raise ValueError(\"Widget 'endpoint_payload' is required (JSON string).\")\n",
    "\n",
    "if not run_folder:\n",
    "    raise ValueError(\"Widget 'run_folder' is required (base volume path for this run).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa1d9f63-97ca-483c-93a6-26c1b8fe7bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Parse payload from step 2\n",
    "# --------------------------------------------------------------------\n",
    "try:\n",
    "    payload: Mapping[str, Any] = json.loads(endpoint_payload_raw)\n",
    "except json.JSONDecodeError as exc:\n",
    "    raise ValueError(f\"Failed to parse 'endpoint_payload' as JSON: {endpoint_payload_raw!r}\") from exc\n",
    "\n",
    "endpoint_id = payload.get(\"id\")\n",
    "endpoint = payload.get(\"endpoint\")\n",
    "params = payload.get(\"params\") or {}\n",
    "job_settings = payload.get(\"job_settings\") or {}\n",
    "\n",
    "if endpoint_id is None:\n",
    "    raise ValueError(\"Endpoint payload is missing required field 'id'.\")\n",
    "if not endpoint:\n",
    "    raise ValueError(\"Endpoint payload is missing required field 'endpoint'.\")\n",
    "\n",
    "logger.info(\"Starting download for endpoint id=%s\", endpoint_id)\n",
    "logger.info(\"Endpoint URL: %s\", endpoint)\n",
    "logger.info(\"Run folder: %s\", run_folder)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Determine endpoint-specific folder\n",
    "# --------------------------------------------------------------------\n",
    "# Allow overriding the subfolder name via job_settings[\"output_subfolder\"]\n",
    "output_subfolder = job_settings.get(\"output_subfolder\") or str(endpoint_id)\n",
    "download_folder = f\"{run_folder}/{output_subfolder}\"\n",
    "\n",
    "# Ensure folder exists\n",
    "dbutils.fs.mkdirs(download_folder)\n",
    "logger.info(\"Download folder for endpoint id=%s: %s\", endpoint_id, download_folder)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Build requests.Session and apply job_settings\n",
    "# --------------------------------------------------------------------\n",
    "session = requests.Session()\n",
    "\n",
    "# Optional headers map in job_settings[\"headers\"]\n",
    "headers = job_settings.get(\"headers\")\n",
    "if isinstance(headers, dict):\n",
    "    session.headers.update(headers)\n",
    "    logger.info(\"Applied %d custom headers from job_settings.\", len(headers))\n",
    "\n",
    "# Optional bearer token support\n",
    "bearer_token = job_settings.get(\"bearer_token\")\n",
    "auth_header_name = job_settings.get(\"auth_header\", \"Authorization\")\n",
    "if bearer_token:\n",
    "    session.headers[auth_header_name] = f\"Bearer {bearer_token}\"\n",
    "    logger.info(\"Applied bearer token using header '%s'.\", auth_header_name)\n",
    "\n",
    "# Timeouts / retries / backoff with sensible defaults\n",
    "connect_timeout = float(job_settings.get(\"connect_timeout\", 30.0))\n",
    "read_timeout = float(job_settings.get(\"read_timeout\", 300.0))\n",
    "max_attempts = int(job_settings.get(\"max_attempts\", 5))\n",
    "base_delay = float(job_settings.get(\"base_delay\", 1.0))\n",
    "backoff_factor = float(job_settings.get(\"backoff_factor\", 2.0))\n",
    "\n",
    "# Pagination options (e.g. job_settings[\"pagination_key\"] = \"meta.next\")\n",
    "options: dict[str, str] = {}\n",
    "pagination_key = job_settings.get(\"pagination_key\")\n",
    "if pagination_key:\n",
    "    options[\"pagination_key\"] = pagination_key\n",
    "\n",
    "logger.info(\n",
    "    \"Download config → timeout=(%.1f, %.1f), max_attempts=%d, base_delay=%.1f, backoff_factor=%.1f, pagination_key=%r\",\n",
    "    connect_timeout,\n",
    "    read_timeout,\n",
    "    max_attempts,\n",
    "    base_delay,\n",
    "    backoff_factor,\n",
    "    pagination_key,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Call shared download utility\n",
    "# --------------------------------------------------------------------\n",
    "pages_downloaded = download_endpoint_to_volume(\n",
    "    session=session,\n",
    "    endpoint=str(endpoint),\n",
    "    params=params or None,\n",
    "    options=options or None,\n",
    "    download_folder=download_folder,\n",
    "    timeout=(connect_timeout, read_timeout),\n",
    "    max_attempts=max_attempts,\n",
    "    base_delay=base_delay,\n",
    "    backoff_factor=backoff_factor,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Completed download for endpoint id=%s → %d pages saved under %s\",\n",
    "    endpoint_id,\n",
    "    pages_downloaded,\n",
    "    download_folder,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Return result summary\n",
    "# --------------------------------------------------------------------\n",
    "result_payload = {\n",
    "    \"endpoint_id\": endpoint_id,\n",
    "    \"endpoint\": endpoint,\n",
    "    \"download_folder\": download_folder,\n",
    "    \"pages_downloaded\": pages_downloaded,\n",
    "}\n",
    "\n",
    "dbutils.notebook.exit(json.dumps(result_payload))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. download api",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
